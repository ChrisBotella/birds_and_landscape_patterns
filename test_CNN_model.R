# created on 26/01/20


# Following lines INSTALL MXNET
cran <- getOption("repos")
cran["dmlc"] <- "https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/"
options(repos = cran)
install.packages("mxnet")

# CHANGE HERE to the local directory 
# where lies the file data_birds_landscape_CNN.RData
# In this directory will be saved the following files, generated by this script: 
# - The reshaped data (Xarray and Yarray)
# - The model symbolic architecture (test_0-symbol.json)
# - The model parameters at each epoch of the lerning process (test_0-XXX.params) 
# - The loss metrics across epochs in a CSV file (test_0_losses.csv)
dataDir = "C:/Users/user/pCloud local/boulot/data/birds_landscape_martin"

# load packages
require(mxnet)
require(raster)
require(ggplot2)

#####
# Reshape data
#####

load(paste(dataDir,'/data_birds_landscape_CNN.RData',sep=""))

setwd(dataDir)

newBirdsArray = array( unlist(birds_array[1,,]) , dim=dim(birds_array[1,,]) )
dimnames(newBirdsArray) = dimnames(birds_array)[2:3]

inY = colnames(newBirdsArray)
inX = as.character(dimnames(raster_array)[3][[1]])
inXandY = intersect(inX,inY)

# check that all supposedly na rasters are indeed NAs
if(F){
  naIds = which(dimnames(raster_array)[3][[1]]%in%raster_with_na)
  verif=rep(NA,length(naIds))
  k=1
  for (i in naIds){
    verif[k]=sum(is.na(as.vector(raster_array[,,i])))>0
    k=k+1
  }
  sum(verif)==length(verif)
}

inXandYnotNa = setdiff(inXandY,raster_with_na)


Yarray= newBirdsArray[,dimnames(newBirdsArray)[2][[1]]%in%inXandYnotNa]

#Modification of Yarray
#For binomial analyses only
Yarray[Yarray > 0] <- 1
############################

#To subset to species identified as good indicators species of landscape (forest) changes
#Positive (forest "lovers")
Yarray <- Yarray[c("PARATE","PHYSIB","DENMED","REGREG","PARPAL","COCCOC","CERFAM"),]
#Negative (forest "haters")
Yarray <- Yarray[c("PERPER","ALAARV","MOTFLA"),]
#Both
Yarray <- Yarray[("PARATE","PHYSIB","DENMED","REGREG","PARPAL","COCCOC","CERFAM",
                 "PERPER","ALAARV","MOTFLA"),]

Xarray= raster_array[,,dimnames(raster_array)[3][[1]]%in%inXandYnotNa] 

dim(Xarray) = c(dim(Xarray)[1],dim(Xarray)[2],1,dim(Xarray)[3])

setwd(dataDir)
saveRDS(Yarray,'Yarray')
saveRDS(Xarray,'Xarray')


MainDevice = mx.cpu()


#####
# Functions
#####

# Returns the list of derivative of the loss with respect to each parameter
# grouped and named by model parts. ex : "conv1_weight"
get.all.gradients = function(model,X,y,executor,X_name="data",y_name="label",
                             device=MainDevice){
  # on lui donne les paramètres du modèle 
  mx.exec.update.arg.arrays(executor , model$arg.params,match.name = T )
  mx.exec.update.aux.arrays(executor, model$aux.params, match.name=T)
  # on lui donne les données d'entrée (et de sortie pour la loss)
  command = paste('mx.exec.update.arg.arrays(executor,list(',
                  X_name,
                  '=mx.nd.array(X),',
                  y_name,
                  '=mx.nd.array(y)),match.name=T)',
                  sep="")
  eval(parse(text= command))
  mx.exec.forward(executor)
  mx.exec.backward(executor)
  return(list(executor$grad.arrays[!names(executor$grad.arrays)%in%c(X_name,y_name)],executor$ref.aux.arrays))
}


# personal prediction service (compatible with 2D (vector) and 4D (3-dim arrays) input data, as well as uni and multilabel output)
predict.executor = function(predictor.symbol,params,Array,max.compute.size=100,aux.params=NULL,verbose=F,devices=MainDevice){
  if(length(dim(Array))==4){
    if(verbose){print('Assume colmajor, detect 4 dimensional input')}
    N = dim(Array)[4]
    first_dim = dim(Array)[1:3]
  }else{
    if(verbose){print('Assume colmajor, detect 2 dimensional input')}
    N = dim(Array)[2]
    first_dim = dim(Array)[1]
  }
  n.pass = - (-N %/% max.compute.size)
  rest = N %% max.compute.size
  
  # We compute predictions per data slice
  for(i in 1:n.pass){
    not.complete = rest>0 & i==n.pass
    # We create an executor for computing predictions on the data slice
    prediction = mx.simple.bind(predictor.symbol,
                                ctx=MainDevice,
                                data= c(first_dim,not.complete * rest + (!not.complete) * max.compute.size))
    # We provide the input data to the executor
    if(not.complete){band=(N-rest+1):N}else{band = (i-1)*max.compute.size + 1:max.compute.size}
    # arguments parameters are the model weights
    if(length(dim(Array))==4){
      mx.exec.update.arg.arrays(prediction,c(list(data=mx.nd.array(Array[,,,band,drop=F])), params),match.name=T)
    }else{
      mx.exec.update.arg.arrays(prediction,c(list(data=mx.nd.array(Array[,band,drop=F])),params),match.name=T)
    }
    # auxiliary parameters are hyper-parameters for which gradient is not computed 
    # They may be used as global statistics, especially for adaptative SGD algorithms 
    if(!is.null(aux.params)){mx.exec.update.aux.arrays(prediction,aux.params,match.name=T)}
    
    
    # is.train=F must be specified for prediction
    # Batchnorm is then applied with the current global statistics of aux.params
    # and those aux.params are unaffected by the current mini-batch
    mx.exec.forward(prediction,is.train=F)
    if(i==1){
      # first prediction slice
      p = as.array(prediction$outputs[[1]])
    }else{
      # concatenate to previous slices
      if(is.null(dim(p))){
        p = c(p, as.array(prediction$outputs[[1]]) )
      }else{
        if(i==2 & verbose){print('detect multilabel, returns prediction matrix')}
        p = cbind( p , as.array(prediction$outputs[[1]]) )
      }
    }
    gc(reset=T)
  }
  return(p)
}

# Loss executor
loss.executor = function(predictor.symbol,params,Array,y,max.compute.size=100,aux.params=NULL,verbose=F,devices=MainDevice){
  if(length(dim(Array))==4){
    if(verbose){print('Assume colmajor, detect 4 dimensional input')}
    N = dim(Array)[4]
    first_dim = dim(Array)[1:3]
  }else{
    if(verbose){print('Assume colmajor, detect 2 dimensional input')}
    N = dim(Array)[2]
    first_dim = dim(Array)[1]
  }
  n.pass = - (-N %/% max.compute.size)
  rest = N %% max.compute.size
  
  # We compute predictions per data slice
  for(i in 1:n.pass){
    not.complete = rest>0 & i==n.pass
    # We provide the input data to the executor
    if(not.complete){band=(N-rest+1):N}else{band = (i-1)*max.compute.size + 1:max.compute.size}
    
    # We create an executor for computing predictions on the data slice
    prediction = mx.simple.bind(predictor.symbol,
                                ctx=devices,
                                data= c(first_dim,length(band)),
                                label = c(dim(y)[1],length(band)))
    # arguments parameters are the model weights
    if(length(dim(Array))==4){
      mx.exec.update.arg.arrays(prediction,c(list(data=mx.nd.array(Array[,,,band,drop=F]),label=mx.nd.array(y[,band,drop=F])), params),match.name=T)
    }else{
      mx.exec.update.arg.arrays(prediction,c(list(data=mx.nd.array(Array[,band,drop=F]),label=mx.nd.array(y[,band,drop=F])),params),match.name=T)
    }
    # auxiliary parameters are hyper-parameters for which gradient is not computed 
    # They may be used as global statistics, especially for adaptative SGD algorithms 
    if(!is.null(aux.params)){mx.exec.update.aux.arrays(prediction,aux.params,match.name=T)}
    
    # is.train=F must be specified for prediction
    # Batchnorm is then applied with the current global statistics of aux.params
    # and those aux.params are unaffected by the current mini-batch
    mx.exec.forward(prediction,is.train=F)
    if(i==1){
      # first loss slice
      lossVec = as.array(prediction$outputs[[1]])
    }else{
      # concatenate to previous slices
      if(is.null(dim(lossVec))){
        lossVec = c(lossVec, as.array(prediction$outputs[[1]]) )
      }else{
        if(i==2 & verbose){print('detect multilabel, returns prediction matrix')}
        lossVec = cbind( lossVec , as.array(prediction$outputs[[1]]) )
      }
    }
    gc(reset=T)
  }
  return(lossVec)
}



plot.metrics = function(tab,it,trainLossBounds=c(0,1)){
  cols=colnames(tab)
  cd=tab$it<=it
  ylim_r=range(c(tab$train.loss[cd],tab$valid.loss[cd],trainLossBounds),na.rm = T)
  d=data.frame(its =c(0:it,0:it,0,it,0,it),
               val=c(tab$train.loss[cd],tab$valid.loss[cd],trainLossBounds[1],trainLossBounds[1],trainLossBounds[2],trainLossBounds[2]),
               curve= c(rep("train loss",length(tab$train.loss[cd])),rep("validation loss",length(tab$valid.loss[cd])),"saturated loss","saturated loss","prior loss","prior loss"))
  pl=ggplot(d,aes(x=its,y=val,group=curve,colour=curve))+geom_point()+geom_line()+coord_cartesian(ylim = ylim_r) +ylab('Mean train loss / accuracy')+xlab('Number of epochs')+theme_bw()
  print(pl)
}

#####
# Birds forest landscape data
#####

setwd(dataDir)

Xarray= readRDS("Xarray")
Yarray = readRDS("Yarray")


nValid = 500
nTest = 500

bag = 1:dim(Yarray)[2]
trainSample = sample(bag,dim(Yarray)[2]-nValid-nTest)
validSample = sample( setdiff(bag,trainSample) , nValid )
testSample = setdiff( setdiff(bag,trainSample) , validSample)

Xtrain = Xarray[,,,trainSample,drop=F]
Xvalid = Xarray[,,,validSample,drop=F]
Xtest = Xarray[,,,testSample,drop=F]

Ytrain = Yarray[,trainSample,drop=F]
Yvalid = Yarray[,validSample,drop=F]
Ytest = Yarray[,testSample,drop=F]

#####
# Design CNN model
#####


n_labels = dim(Ytrain)[1]

data = mx.symbol.Variable(name = "data")
label = mx.symbol.Variable(name="label")
# 1st convolutional layer
conv1 = mx.symbol.Convolution(data = data, kernel = c(3, 3), 
                              pad = c(2, 2),
                              num_filter = 32,
                              name = "conv1")
relu1 = mx.symbol.Activation(data = conv1,
                             act_type = "relu",
                             name = "relu1")
pool1 = mx.symbol.Pooling(data = relu1,
                          pool_type = "max",
                          kernel = c(4,4),
                          stride=c(4,4),
                          name = "pool1")
bn1   = mx.symbol.BatchNorm(name="bn1",
                            data=pool1,
                            fix_gamma=F,
                            momentum = 0.9,
                            eps = 2e-5)
# 2nd convolutional layer
conv2 = mx.symbol.Convolution(data = bn1, 
                              kernel = c(3, 3),
                              pad = c(2, 2),
                              num_filter = 32, 
                              name = "conv2")
relu2 = mx.symbol.Activation(data = conv2, 
                             act_type = "relu", 
                             name = "relu2")
pool2 = mx.symbol.Pooling(data = relu2, 
                          pool_type = "max",
                          kernel = c(5, 5),
                          stride =c(5, 5), 
                          name = "pool2")

# Flattening
flatten = mx.symbol.Flatten(data = pool2, 
                            name = "flatten")
bn2   = mx.symbol.BatchNorm( name="bn2",
                             data=flatten,
                             fix_gamma=F,
                             momentum=0.9 ,
                             eps= 2e-5)

# Fully connected layer
fc1 = mx.symbol.FullyConnected(data = bn2,
                               num_hidden = 200,
                               name = "fc1")
rel3 = mx.symbol.Activation(data = fc1,
                            act_type = "relu",
                            name = "relu3")
bn4   = mx.symbol.BatchNorm( name="bn4",
                             data=rel3,
                             fix_gamma=F,
                             momentum=0.9 ,
                             eps= 2e-5)

# Linear Predictor
out = mx.symbol.FullyConnected(data = bn4, num_hidden = n_labels, name = "out")

# Poisson negative log-likelihood
loss = mx.symbol.MakeLoss(data= mx.symbol.exp(out) - label * out , name="poisson")

symbolicModelOutputs = list(loss=loss,log_prediction = out )

#####
# Learn and save CNN model
#####

# Training parameters
n_it= 3
batch.size = 32
saveDir = dataDir
lr = rep(5e-6,n_it+1)
modelName = "test_0"
loadModel = F

if(loadModel ==F){
  # Randomly Initialize the model weights
  mx.set.seed(2019)
  model= mx.model.FeedForward.create(symbol=symbolicModelOutputs$loss[[1]],
                                     X=Xtrain,y=Ytrain,                
                                     ctx=mx.cpu(),begin.round=1,num.round = 1,
                                     learning.rate=0.,
                                     initializer = mx.init.uniform(0.03),
                                     array.layout = "colmajor")
}else{
  # OR load pre-trained model
  setwd(saveDir)
  model = mx.model.load("finish2",iteration = 6)
}

# Initialize the moving gradient average (For SGD/momentum algorithm) to 0
Names = names(model$arg.params)
E_G = lapply(Names,function(arg) 0.*as.array(model$arg.params[arg][[1]]) )
names(E_G)=Names

# Table of loss results per epoch for plot
tab = data.frame( it = 0:n_it,train.loss = NA,valid.loss=NA)

# prediction sample 
N = dim(Ytrain)[2]
samplo = sample(1:N,500)
priorLoss = - log(1/ dim(Ytrain)[1]) /dim(Ytrain)[1] # prior Loss for plot
n.batch = -( -N %/% batch.size)
it=0
while (it<=n_it){
  print(paste('it :',it))
  
  # PREDICT on train
  logCountsPredTrain = predict.executor(symbolicModelOutputs$log_prediction[[1]],
                       model$arg.params,
                       Xtrain[,,,samplo,drop=F],
                       devices = MainDevice,
                       aux.params = model$aux.params,
                       max.compute.size = 200)
  # COMPUTE MEAN TRAIN LOSS
  lossTrain = loss.executor(predictor.symbol = symbolicModelOutputs$loss[[1]],
                       params =model$arg.params,
                       Array = Xtrain[,,,samplo,drop=F],
                       y = Ytrain[,samplo,drop=F], 
                       devices = MainDevice,
                       aux.params = model$aux.params,
                       max.compute.size = 200)
  if(sum(is.na(lossTrain))>0){lossTrain[is.na(lossTrain)] = 0}
  if(sum(is.infinite(lossTrain))>0){
    print('Infinite terms in the train loss')
    tab$train.loss[tab$it==it]= NA
  }else{
    meanLoss = mean(as.vector(lossTrain))
    print(paste('Mean train Loss :',meanLoss))
    tab$train.loss[tab$it==it]= meanLoss
  }
  
  # PREDICT on validation
  logCountsPredValid = predict.executor(symbolicModelOutputs$log_prediction[[1]],
                       model$arg.params,
                       Xvalid,
                       devices = mx.cpu(),
                       aux.params = model$aux.params,
                       max.compute.size = 200)
  # COMPUTE MEAN VALIDATION LOSS
  lossValid = loss.executor(predictor.symbol = symbolicModelOutputs$loss[[1]],
                       params =model$arg.params,
                       Array = Xvalid,
                       y = Yvalid, 
                       devices = mx.cpu(),
                       aux.params = model$aux.params,
                       max.compute.size = 200)
  
  if(sum(is.na(lossValid))>0){lossValid[is.na(lossValid)] = 0}
  if(sum(is.infinite(lossValid))>0){
    print('Infinite terms in the validation loss')
    tab$valid.loss[tab$it==it]= NA
  }else{
    meanLoss = mean(as.vector(lossValid))
    print(paste('Mean validation Loss :',meanLoss))
    tab$valid.loss[tab$it==it]= meanLoss
  }
  
  # PLOT LOSS
  plot.metrics(tab = tab,it = it)
  
  #trainLoss = tab$tr.loss[tab$it<=it]
  #validLoss = tab$valid.loss[tab$it<=it]
  #ylim_r=range(c(trainLoss,0,priorLoss,validLoss),na.rm = T)
  #d=data.frame(its =c(0:it,0:it,0,it,0,it),
  #             val=c(trainLoss,validLoss,0,0,priorLoss,priorLoss),
  #             curve= c(rep("train loss",length(trainLoss)),rep("validation loss",length(validLoss)),"saturated loss","saturated loss","prior loss","prior loss"))
  #pl=ggplot(d,aes(x=its,y=val,group=curve,colour=curve))+geom_point()+geom_line()+coord_cartesian(ylim = ylim_r) +ylab('Mean train loss')+xlab('Number of epochs')+theme_bw()
  #print(pl)
  
  # Tirage des mini-batchs
  sampli = 1:N
  batchs.list = list()
  for(k in 1:n.batch){
    reduce = (length(sampli)<batch.size) * (batch.size-length(sampli))
    batchs.list[[k]] = sample(sampli , batch.size - reduce , replace=F)
    sampli = sampli[!(sampli%in% batchs.list[[k]]) ]
  }
  
  executor = mx.simple.bind(model$symbol,
                            data=dim(Xtrain[,,,batchs.list[[1]],drop=F]),
                            grad.req ="write",
                            ctx=MainDevice)
  
  # pass over each mini-batch
  for(k in 1:n.batch){
    elems = batchs.list[[k]]
    if(length(elems)<batch.size){
      executor = mx.simple.bind(model$symbol,data=dim(Xtrain[,,,elems,drop=F]),grad.req ="write",ctx=MainDevice)
    }
    cat('\r  Process ...',100*k/n.batch,' %                               \r')
    flush.console()
    GradAndAux = get.all.gradients(model,Xtrain[,,,elems,drop=F],Ytrain[,elems,drop=F],executor)
    
    for(arg in names(GradAndAux[[2]])){
      # Update each auxiliary parameter set by extracting them from GradAndAux
      # We copy those components to the CPU 
      # because that is where our model components are located
      model$aux.params[arg][[1]] = mx.nd.copyto(GradAndAux[[2]][arg][[1]],mx.cpu())
    }
    G= GradAndAux[[1]]
    for(arg in Names){
      # We compute parameters delta through R 
      # because I had a crash with Mxnet from an unknown reason ... 
      E_G[arg][[1]] = 0.9 * E_G[arg][[1]] + lr[it+1]* as.array(G[arg][[1]])
      # We compute parameters update through Mxnet
      model$arg.params[arg][[1]] = model$arg.params[arg][[1]] -  mx.nd.array( E_G[arg][[1]],ctx=mx.cpu())
    }
    gc(reset=T)
  }
  it=it+1
  setwd(saveDir)
  mx.model.save(model,modelName,iteration=it)
  write.table(tab,paste(modelName,"_losses.csv",sep=""),sep=";",row.names=F,col.names=T)
}


